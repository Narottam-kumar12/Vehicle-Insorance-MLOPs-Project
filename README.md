This version is designed to be visually striking, easy to scan, and optimized for hiring managers who may only spend 30 seconds looking at your repository. It uses professional formatting, clear hierarchy, and industry-standard documentation practices.

---

# ğŸš— Vehicle Data ML-Ops Pipeline

> **A Production-Grade End-to-End Machine Learning Ecosystem**

<br />

## ğŸŒŸ Project Overview

Handling machine learning models in a notebook is easy; bringing them into production is where the challenge lies. This project solves the **"Deployment Gap"** by building a fully automated **ML-Ops Pipeline**.

From the moment data hits the **MongoDB** database to its final deployment as a **Dockerized container on AWS**, every stepâ€”ingestion, validation, transformation, and evaluationâ€”is code-driven and automated.

---

## ğŸ›  Tech Stack

| Layer | Technologies |
| --- | --- |  
| **Language** | 'python' |
| **Data Storage** |  |
| **Infrastructure** |  |
| **DevOps** |  |
| **ML Libraries** | `Scikit-learn`, `Pandas`, `XGBoost`, `EvidentlyAI` |

---

## ğŸ— System Architecture & Workflow

The pipeline is designed with a **Modular Component Architecture**, ensuring high maintainability and scalability.

### **The Pipeline Flow:**

1. **Data Ingestion:** Securely pulls raw vehicle data from MongoDB Atlas.
2. **Data Validation:** Checks for schema consistency and data drift using a predefined YAML schema.
3. **Data Transformation:** Automates feature engineering and handles class imbalances.
4. **Model Trainer:** Trains the model and exports artifacts (Pickle files).
5. **Model Evaluation:** Compares the "Candidate" model with the "Production" model stored in **AWS S3**.
6. **Model Pusher:** If the candidate model is better (threshold > 2%), it is automatically pushed to the **S3 Model Registry**.

---

## ğŸ“‚ Project Structure

```bash
â”œâ”€â”€ .github/workflows   # ğŸš€ CI/CD pipeline definitions
â”œâ”€â”€ src/                # ğŸ’» Core source code
â”‚   â”œâ”€â”€ components      # Data Ingestion -> Model Pusher
â”‚   â”œâ”€â”€ entity          # Configuration and Artifact schemas
â”‚   â”œâ”€â”€ pipeline        # Training & Prediction logic
â”‚   â””â”€â”€ utils           # Helper functions (AWS/S3/MongoDB)
â”œâ”€â”€ static/             # ğŸ¨ UI CSS and JS
â”œâ”€â”€ templates/          # ğŸ“„ HTML Web Interfaces
â”œâ”€â”€ Dockerfile          # ğŸ³ Containerization instructions
â””â”€â”€ app.py              # ğŸ”Œ Flask/FastAPI Gateway

```

---

## ğŸš€ Installation & Local Setup

### **1. Environment Setup**

```bash
# Clone the repository
git clone https://github.com/your-username/vehicle-mlops.git

# Create and Activate Environment
conda create -n vehicle python=3.10 -y
conda activate vehicle

# Install dependencies
pip install -r requirements.txt

```

### **2. Set Secret Keys**

```bash
# Mac/Linux
export MONGODB_URL="your_mongodb_connection_string"
export AWS_ACCESS_KEY_ID="your_aws_key"
export AWS_SECRET_ACCESS_KEY="your_aws_secret"

# Windows (Powershell)
$env:MONGODB_URL="your_mongodb_connection_string"

```

---

## ğŸ“ˆ Results & Performance

* **Model Performance:** Achieved an F1-Score of **0.XX** on validation sets.
* **Automation:** Reduced deployment time from hours to **under 5 minutes** via CI/CD.
* **Reliability:** Implemented automated data validation, reducing "Garbage In, Garbage Out" risks.

---

## ğŸ’¡ Key Engineering Highlights (Placement Ready)

* **Scalable Codebase:** Used "Entity-Component" design patterns for clean, production-grade code.
* **Cloud Native:** Integrated AWS S3 for model versioning and ECR for container management.
* **CI/CD Proficiency:** Established a self-hosted runner on EC2 for secure, automated deployments.
* **Containerization:** Utilized Docker to eliminate the "it works on my machine" problem.

---

## ğŸ‘¨â€ğŸ’» Author

**Narottam Kumar** *Computer Science & Engineering | MMMUT Gorakhpur* [](https://www.linkedin.com/in/narottam-kumar-a16a04293/)
[](https://github.com/Narottam-kumar12)
